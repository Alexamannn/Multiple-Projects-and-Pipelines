{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seo optimizationT.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzVSVNsdLto6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f55aaf-10a8-4ce9-d9bf-fcd116d44993"
      },
      "source": [
        "from googlesearch import search\r\n"
      ],
      "execution_count": 500,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-12 01:05:38--  https://raw.githubusercontent.com/Alexamannn/UniCredit-Stock-Predictor--machine-Learning-Auto-Arima/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     113  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-12 01:05:38 (6.54 MB/s) - ‘requirements.txt’ saved [113/113]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4RKQAvfr-Lp",
        "outputId": "12c7c8aa-10cd-4af9-d4f6-527a7158b62f"
      },
      "source": [
        "query_list=[input()]\r\n",
        "other=input()"
      ],
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "come crrea\n",
            "visulaitic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3k0T0mTMvTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c84f02-cc86-4406-a0ff-252c32c91a00"
      },
      "source": [
        "query_list"
      ],
      "execution_count": 493,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['come crrea']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 493
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esI-MVt-L-zh"
      },
      "source": [
        "k=[]\r\n",
        "for j in query_list:\r\n",
        "    for i in search(j,  tld='it', lang='it', num=10, start=0, stop=30):\r\n",
        "        k.append(i)"
      ],
      "execution_count": 494,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCSjZzlMQse4"
      },
      "source": [
        "import pandas as pd\r\n",
        "h=pd.DataFrame(k)\r\n",
        "h.columns=[\"hi\"]\r\n",
        "ff=h['hi'].str.split('/',expand=True)"
      ],
      "execution_count": 495,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai-OxlhITIY6"
      },
      "source": [
        "def getIndexes(dfObj, value):\r\n",
        "    listOfPos = list()\r\n",
        "    result = dfObj.isin([value])\r\n",
        "    seriesObj = result.any()\r\n",
        "    columnNames = list(seriesObj[seriesObj == True].index)\r\n",
        "    for col in columnNames:\r\n",
        "        rows = list(result[col][result[col] == True].index)\r\n",
        "        for row in rows:\r\n",
        "            listOfPos.append((row, col))\r\n",
        "    return listOfPos"
      ],
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN4hyf9HTQk7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "883c5b9b-5366-4748-adfa-8407d97955c6"
      },
      "source": [
        "postion = getIndexes(ff,\"www.theinformationlab.it\")\r\n",
        "other1=getIndexes(ff,other)\r\n",
        "infolab=postion[0][0]\r\n",
        "first=k[0]\r\n",
        "others=other1[0][0]"
      ],
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-497-dc7000e659e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpostion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetIndexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"www.theinformationlab.it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mother1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetIndexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minfolab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpostion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spvhV82kXSPO"
      },
      "source": [
        "def images(dir,website):\r\n",
        " site = website\r\n",
        " response = requests.get(site)\r\n",
        " dirpath = os.path.join('./', dir)\r\n",
        " os.mkdir(dirpath)\r\n",
        " soup = BeautifulSoup(response.text, 'html.parser')\r\n",
        " img_tags = soup.find_all('img')\r\n",
        " urls = [img['src'] for img in img_tags]\r\n",
        " for url in urls:\r\n",
        "    filename = re.search(r'/([\\w_-]+[.](jpg|gif|png))$', url)\r\n",
        "    if not filename:\r\n",
        "         print(\"Regex didn't match with the url: {}\".format(url))\r\n",
        "         continue\r\n",
        "    with open(\"/content/\"+dir+\"/\"+filename.group(1), 'wb') as f:\r\n",
        "        if 'http' not in url:\r\n",
        "            url = '{}{}'.format(site, url)\r\n",
        "        response = requests.get(url)\r\n",
        "        f.write(response.content)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP5d7T0oZTYN"
      },
      "source": [
        "images(\"inforlab\",k[pos])\r\n",
        "images(\"other\",k[others])\r\n",
        "images(\"first\",first)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBhNzM-tdmKm"
      },
      "source": [
        "def fn(dir):   \r\n",
        "    k=[]\r\n",
        "    file_list=os.listdir(r\"/content/\"+dir)\r\n",
        "    return(file_list)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWtJOg67_cNS"
      },
      "source": [
        "z=fn(\"inforlab\")\r\n",
        "z1=fn(\"first\")\r\n",
        "zo=fn(\"other\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6j6c3qU_sFJ"
      },
      "source": [
        "images=pd.DataFrame([z,z1,zo])\r\n",
        "images=images.T\r\n",
        "images.columns=\"infolab\",\"first\",\"other\"\r\n",
        "images.to_csv(\"/content/data/images.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUjKmCMZA8Wj"
      },
      "source": [
        "import re\r\n",
        "def resp(url):\r\n",
        " response = requests.get(url)\r\n",
        " soup = BeautifulSoup(response.text, 'html.parser')\r\n",
        " a_text = soup.find_all('p')\r\n",
        " text=str(a_text)\r\n",
        " text= ''.join([i for i in text if not i.isdigit()])\r\n",
        " text = re.sub('[><+()-_!/\\!@#$]', '', text)\r\n",
        " text=str(text)\r\n",
        " return text\r\n",
        "\r\n",
        "def a_text(url):\r\n",
        " response = requests.get(url)\r\n",
        " soup = BeautifulSoup(response.text, 'html.parser')\r\n",
        " a_text = soup.find_all('p')\r\n",
        " text=str(a_text)\r\n",
        " return a_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_yCyCiXXZJ9"
      },
      "source": [
        "infotext=resp(k[pos])\r\n",
        "infoa_text=a_text(k[pos])\r\n",
        "firsttext=resp(k[0])\r\n",
        "firsta_text=a_text(k[0])\r\n",
        "othertext=resp(k[others])\r\n",
        "othera_text=a_text(k[others])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avMEDIJBHQaa"
      },
      "source": [
        "from nltk import tokenize\r\n",
        "from operator import itemgetter\r\n",
        "import math\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "stop_words = set(stopwords.words('italian'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luFTxwH5YunL"
      },
      "source": [
        "def check_sent(word, sentences): \r\n",
        "    final = [all([w in x for w in word]) for x in sentences] \r\n",
        "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\r\n",
        "    return int(len(sent_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ReKS-VNZX_0"
      },
      "source": [
        "def get_top_n(dict_elem, n):\r\n",
        "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDSzSU_GEogD"
      },
      "source": [
        "info_words = infotext.split()\r\n",
        "info_word_length = len(info_words)\r\n",
        "info_sentences = tokenize.sent_tokenize(str(infoa_text))\r\n",
        "info_sent_len = len(info_sentences)\r\n",
        "\r\n",
        "first_words = firsttext.split()\r\n",
        "first_word_length = len(first_words)\r\n",
        "first_sentences = tokenize.sent_tokenize(str(firsta_text))\r\n",
        "first_sent_len = len(first_sentences)\r\n",
        "\r\n",
        "other_words = othertext.split()\r\n",
        "other_word_length = len(other_words)\r\n",
        "other_sentences = tokenize.sent_tokenize(str(othera_text))\r\n",
        "other_sent_len = len(other_sentences)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K597HTHSLVES"
      },
      "source": [
        "info_tf_score = {}\r\n",
        "for each_word in info_words:\r\n",
        "    each_word = each_word.replace('.','')\r\n",
        "    if each_word not in stop_words:\r\n",
        "        if each_word in info_tf_score:\r\n",
        "            info_tf_score[each_word] += 1\r\n",
        "        else:\r\n",
        "            info_tf_score[each_word] = 1\r\n",
        "\r\n",
        "# Dividing by total_word_length for each dictionary element\r\n",
        "info_tf_score.update((x, y/int(info_word_length)) for x, y in info_tf_score.items())\r\n",
        "print(info_tf_score)\r\n",
        "\r\n",
        "first_tf_score = {}\r\n",
        "for each_word in first_words:\r\n",
        "    each_word = each_word.replace('.','')\r\n",
        "    if each_word not in stop_words:\r\n",
        "        if each_word in first_tf_score:\r\n",
        "            first_tf_score[each_word] += 1\r\n",
        "        else:\r\n",
        "            first_tf_score[each_word] = 1\r\n",
        "\r\n",
        "# Dividing by total_word_length for each dictionary element\r\n",
        "first_tf_score.update((x, y/int(first_word_length)) for x, y in first_tf_score.items())\r\n",
        "print(first_tf_score)\r\n",
        "\r\n",
        "other_tf_score = {}\r\n",
        "for each_word in other_words:\r\n",
        "    each_word = each_word.replace('.','')\r\n",
        "    if each_word not in stop_words:\r\n",
        "        if each_word in other_tf_score:\r\n",
        "            other_tf_score[each_word] += 1\r\n",
        "        else:\r\n",
        "            other_tf_score[each_word] = 1\r\n",
        "\r\n",
        "# Dividing by total_word_length for each dictionary element\r\n",
        "other_tf_score.update((x, y/int(other_word_length)) for x, y in other_tf_score.items())\r\n",
        "print(other_tf_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GxyvwKmLhvq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCPHaxKKLi6q"
      },
      "source": [
        "info_idf_score = {}\r\n",
        "for each_word in info_words:\r\n",
        "    each_word = each_word.replace('.','')\r\n",
        "    if each_word not in stop_words:\r\n",
        "        if each_word in info_idf_score:\r\n",
        "            info_idf_score[each_word] = check_sent(each_word, info_sentences)\r\n",
        "        else:\r\n",
        "            info_idf_score[each_word] = 1\r\n",
        "\r\n",
        "# Performing a log and divide\r\n",
        "info_idf_score.update((x, math.log(int(info_sent_len)/y)) for x, y in info_idf_score.items())\r\n",
        "\r\n",
        "print(info_idf_score)\r\n",
        "\r\n",
        "first_idf_score = {}\r\n",
        "for each_word in first_words:\r\n",
        "    each_word = each_word.replace('.','')\r\n",
        "    if each_word not in stop_words:\r\n",
        "        if each_word in first_idf_score:\r\n",
        "            first_idf_score[each_word] = check_sent(each_word, first_sentences)\r\n",
        "        else:\r\n",
        "            first_idf_score[each_word] = 1\r\n",
        "\r\n",
        "# Performing a log and divide\r\n",
        "first_idf_score.update((x, math.log(int(first_sent_len)/y)) for x, y in first_idf_score.items())\r\n",
        "\r\n",
        "print(first_idf_score)\r\n",
        "\r\n",
        "other_idf_score = {}\r\n",
        "for each_word in other_words:\r\n",
        "    each_word = each_word.replace('.','')\r\n",
        "    if each_word not in stop_words:\r\n",
        "        if each_word in other_idf_score:\r\n",
        "            other_idf_score[each_word] = check_sent(each_word, other_sentences)\r\n",
        "        else:\r\n",
        "            other_idf_score[each_word] = 1\r\n",
        "\r\n",
        "# Performing a log and divide\r\n",
        "other_idf_score.update((x, math.log(int(other_sent_len)/y)) for x, y in other_idf_score.items())\r\n",
        "\r\n",
        "print(other_idf_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy-NjcNULqIw"
      },
      "source": [
        "info_tf_idf_score = {key: info_tf_score[key] * info_idf_score.get(key, 0) for key in info_tf_score.keys()}\r\n",
        "first_tf_idf_score = {key: first_tf_score[key] * first_idf_score.get(key, 0) for key in first_tf_score.keys()}\r\n",
        "other_tf_idf_score = {key: other_tf_score[key] * other_idf_score.get(key, 0) for key in other_tf_score.keys()}\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I5MgxBfLtwU"
      },
      "source": [
        "def get_top_n(dict_elem, n):\r\n",
        "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE1507NkLvKG"
      },
      "source": [
        "infolabtext=(get_top_n(info_tf_idf_score, 100))\r\n",
        "firsttext=(get_top_n(first_tf_idf_score, 100))\r\n",
        "othertext=(get_top_n(other_tf_idf_score, 100))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4HyphkbgcV1"
      },
      "source": [
        "textdf=pd.DataFrame(infolabtext.items())\r\n",
        "textdf.columns=\"infolab\",\"Drop\"\r\n",
        "del textdf['Drop']\r\n",
        "first=pd.DataFrame(firsttext.items())\r\n",
        "first.columns=\"first\",\"drop\"\r\n",
        "del first[\"drop\"]\r\n",
        "other=pd.DataFrame(othertext.items())\r\n",
        "other.columns=\"other\",\"drop\"\r\n",
        "del other[\"drop\"]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqSC4IAijZSv"
      },
      "source": [
        "first[\"other\"]=other[\"other\"]\r\n",
        "first[\"infolab\"]=textdf[\"infolab\"]\r\n",
        "keywords=first\r\n",
        "keywords.to_csv(\"/content/data/seo optimization/keywords.csv\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}